{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e81d83-dc64-407a-bb89-af0bf34d80fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "source": [
    "print('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f88c832-feb5-43cd-85c1-bc734d158b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_467514/809267710.py:68: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"/ibex/project/c2205/AMR_dataset_peijun/integrate/final_all_additional_note_feb14.csv\",  usecols = use_cols, header=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30278, 34)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from modelscope import snapshot_download, AutoTokenizer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    set_seed \n",
    ")\n",
    "import tensorboard\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Add argument for antimicrobial labels\n",
    "# parser.add_argument('--labels', nargs='+', help='Targeted antimicrobial.')\n",
    "# args = parser.parse_args()\n",
    "# labels_list = args.labels\n",
    "# labels_list = [\"resistance_nitrofurantoin\", \"resistance_sulfamethoxazole\", \"resistance_ciprofloxacin\", \"resistance_levofloxacin\"]\n",
    "\n",
    "labels_list = ['resistance_nitrofurantoin']\n",
    "\n",
    "# Set seed for reproducibility\n",
    "\n",
    "set_seed(42)  \n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "print('hi')\n",
    "\n",
    "# Columns to use from the dataset\n",
    "use_cols = ['age', 'race', 'veteran', 'gender', 'BMI', 'previous_antibiotic_exposure_cephalosporin',\n",
    "       'previous_antibiotic_exposure_carbapenem',\n",
    "       'previous_antibiotic_exposure_fluoroquinolone',\n",
    "       'previous_antibiotic_exposure_polymyxin',\n",
    "       'previous_antibiotic_exposure_aminoglycoside',\n",
    "       'previous_antibiotic_exposure_nitrofurantoin',\n",
    "       'previous_antibiotic_resistance_ciprofloxacin',\n",
    "       'previous_antibiotic_resistance_levofloxacin',\n",
    "       'previous_antibiotic_resistance_nitrofurantoin',\n",
    "       'previous_antibiotic_resistance_sulfamethoxazole','resistance_nitrofurantoin', 'resistance_sulfamethoxazole',\n",
    "       'resistance_ciprofloxacin', 'resistance_levofloxacin', 'source',\n",
    "        'dept_ER', 'dept_ICU',\n",
    "       'dept_IP', 'dept_OP', 'dept_nan',\n",
    "       'Enterococcus_faecium', 'Staphylococcus_aureus',\n",
    "       'Klebsiella_pneumoniae', 'Acinetobacter_baumannii',\n",
    "       'Pseudomonas_aeruginosa', 'Enterobacter', 'organism_other',\n",
    "       'organism_NA', 'additional_note']\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"/ibex/project/c2205/AMR_dataset_peijun/integrate/final_all_additional_note_feb14.csv\",  usecols = use_cols, header=0) \n",
    "data = data.sample(frac=1/50, random_state=42)  # only use a fraction of dataset for debugging \n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "# Separate features and labels\n",
    "features = data.drop(columns=[\"resistance_nitrofurantoin\", \"resistance_sulfamethoxazole\", \"resistance_ciprofloxacin\", \"resistance_levofloxacin\"])\n",
    "labels = data[[\"resistance_nitrofurantoin\", \"resistance_sulfamethoxazole\", \"resistance_ciprofloxacin\", \"resistance_levofloxacin\"]]\n",
    "\n",
    "\n",
    "# Convert CSV rows into NLP model's input format\n",
    "data[\"input\"] = features.apply(lambda row: \"; \".join([f\"{col.replace('_',' ')}:{val}\" for col, val in row.items()]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c32e961e-0cda-4dd1-a793-6d3603dbb284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating for label: resistance_nitrofurantoin\n",
      "Running times:1\n",
      "Trainning datasets length:20556\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 128\u001b[0m\n\u001b[1;32m    126\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(train_messages)\n\u001b[1;32m    127\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(train_df)\n\u001b[0;32m--> 128\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mantibiotics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mantimicrobial\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m len_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_test)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest datasets length:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(len_test))\n",
      "File \u001b[0;32m/ibex/user/xiex/conda-environments/llm4amr/lib/python3.9/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/ibex/user/xiex/conda-environments/llm4amr/lib/python3.9/site-packages/datasets/arrow_dataset.py:3074\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3072\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mhf_tqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m        \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m examples\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbar_total\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3077\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3078\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m   3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[0;32m/ibex/user/xiex/conda-environments/llm4amr/lib/python3.9/site-packages/tqdm/std.py:665\u001b[0m, in \u001b[0;36mtqdm.__new__\u001b[0;34m(cls, *_, **__)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m__):\n\u001b[1;32m    664\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_lock():  \u001b[38;5;66;03m# also constructs lock if non-existent\u001b[39;00m\n\u001b[1;32m    666\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instances\u001b[38;5;241m.\u001b[39madd(instance)\n\u001b[1;32m    667\u001b[0m         \u001b[38;5;66;03m# create monitoring thread\u001b[39;00m\n",
      "File \u001b[0;32m/ibex/user/xiex/conda-environments/llm4amr/lib/python3.9/site-packages/tqdm/std.py:111\u001b[0m, in \u001b[0;36mTqdmDefaultWriteLock.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ibex/user/xiex/conda-environments/llm4amr/lib/python3.9/site-packages/tqdm/std.py:104\u001b[0m, in \u001b[0;36mTqdmDefaultWriteLock.acquire\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lock \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocks:\n\u001b[0;32m--> 104\u001b[0m         \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Download Qwen model\n",
    "# snapshot_download(\"qwen/Qwen2-1.5B-Instruct\", cache_dir=\"./\", revision=\"master\")\n",
    "\n",
    "# Loading tokenizer (map words in sentences into index)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../qwen/Qwen2-1___5B-Instruct/\", use_fast=False, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Load the Qwen model\n",
    "Qwen_model = AutoModelForCausalLM.from_pretrained(\"../qwen/Qwen2-1___5B-Instruct/\",device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "Qwen_model.enable_input_require_grads()\n",
    "\n",
    "# Function to process the data into tokens\n",
    "def process_func(example, antibiotics):\n",
    "    # transfer the message into tokens, perform the masking, padding, and max length cutting\n",
    "    MAX_LENGTH = 318\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    \n",
    "   \n",
    "    feature_str = example[\"input\"]\n",
    "    \n",
    "    # Construct the instruction for the model\n",
    "    instruction = tokenizer(\n",
    "        f\"<|im_start|>system\\nYou are an expert in predicting antibiotic resistance for {antibiotics} based on patient electronic healthe records. Please output the prediction results.<|im_end|>\\n<|im_start|>user\\n{feature_str}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    \n",
    "    # Construct response for the model\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    \n",
    "    \n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    \n",
    "    # Truncate to max length\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels_ids = eval_preds\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        try:\n",
    "            pred_label = int(float(pred.strip()))\n",
    "        except Exception as e:\n",
    "            pred_label = -1\n",
    "        try:\n",
    "            true_label = int(float(label.strip()))\n",
    "        except Exception as e:\n",
    "            true_label = -1\n",
    "        pred_labels.append(pred_label)\n",
    "        true_labels.append(true_label)\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    TP = cm[1][1] if cm.shape[0] > 1 and cm.shape[1] > 1 else 0\n",
    "    TN = cm[0][0] if cm.shape[0] > 0 and cm.shape[1] > 0 else 0\n",
    "    FP = cm[0][1] if cm.shape[0] > 0 and cm.shape[1] > 1 else 0\n",
    "    FN = cm[1][0] if cm.shape[0] > 1 and cm.shape[1] > 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(\"precision:\", precision)\n",
    "    print(\"recall:\", recall)\n",
    "    print(\"f1:\", f1)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over each label to predict\n",
    "for label in labels_list:\n",
    "\n",
    "    X = data['input'].tolist()\n",
    "    Y = labels[label].tolist()\n",
    "\n",
    "    valid_indices = [i for i, y in enumerate(Y) if not np.isnan(y)]\n",
    "\n",
    "    X = [X[i] for i in valid_indices]\n",
    "    Y = [Y[i] for i in valid_indices]\n",
    "\n",
    "    antimicrobial = label.split('_')[-1]\n",
    "\n",
    "\n",
    "    print(f\"Training and evaluating for label: {label}\")\n",
    "    \n",
    "    running_times = 0\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    running_times = running_times + 1\n",
    "    print(\"Running times:\" + str(running_times))\n",
    "\n",
    "\n",
    "    # Function to build messages for the model\n",
    "    def build_messages(x_list, y_list):\n",
    "        messages = []\n",
    "        for feature, output in zip(x_list, y_list):\n",
    "            messages.append({\n",
    "                \"instruction\": f\"You are an expert in prediction of antimicrobial resistance for {antimicrobial}, and you will receive patients’ electronic health record features. Please output the prediction results.\",\n",
    "                \"input\": feature,\n",
    "                \"output\": output,\n",
    "            })\n",
    "        return messages\n",
    "\n",
    "    train_messages = build_messages(x_train, y_train)\n",
    "    test_messages = build_messages(x_test, y_test)\n",
    "\n",
    "    #trainning dataset\n",
    "    len_train = len(y_train)\n",
    "    print(\"Trainning datasets length:\" + str(len_train))\n",
    "    train_df = pd.DataFrame(train_messages)\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    train_dataset = train_ds.map(process_func, fn_kwargs={\"antibiotics\": antimicrobial}, remove_columns=train_ds.column_names)\n",
    "\n",
    "    len_test = len(y_test)\n",
    "    print(\"Test datasets length:\" + str(len_test))\n",
    "    test_df = pd.DataFrame(test_messages)\n",
    "    test_ds = Dataset.from_pandas(test_df)\n",
    "    test_dataset = test_ds.map(process_func, fn_kwargs={\"antibiotics\": antimicrobial}, remove_columns=train_ds.column_names)\n",
    "\n",
    "    # using api in peft (param-effective finetuning)\n",
    "    # config Lora model 1.5B\n",
    "    config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        ],\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "    )\n",
    "    \n",
    "    # Combine Qwen model with LoRA\n",
    "    model = get_peft_model(Qwen_model, config)\n",
    "\n",
    "\n",
    "    # Set training arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./output_my_training_Match_15_gen/\"+ antimicrobial + \"/\"+datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S'),\n",
    "        per_device_train_batch_size=16,  # batch size\n",
    "        per_device_eval_batch_size=2,\n",
    "        fp16=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        gradient_accumulation_steps=1,  \n",
    "        logging_steps=5,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=1e-5,\n",
    "        gradient_checkpointing=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        predict_with_generate=True,\n",
    "        report_to=[\"tensorboard\"], # save the training process to tensorboard\n",
    "        seed=42,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=300,\n",
    "        save_total_limit=2,\n",
    "        dataloader_num_workers=4,  \n",
    "        ddp_find_unused_parameters=False,  # multi GPU\n",
    "        deepspeed=None, \n",
    "        local_rank=int(os.getenv('LOCAL_RANK', -1)), \n",
    "        generation_max_length=10,     # truncate the response length, supposed to be short\n",
    "    )\n",
    "\n",
    "     # Set up the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset = test_dataset,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc7b2d5-4c85-4c2b-9c26-e1b4d23de386",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a913d-9bdd-4bdc-9ad9-8fc74143edf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
